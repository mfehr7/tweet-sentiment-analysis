{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis - World Cup Tweets\n",
        "\n",
        "* **Author:** Mitch Fehr\n",
        "\n",
        "* This notebook creates a neural network using tensorflow, designed for sentiment analysis.\n",
        "* The network is trained on tweets from the 2022 FIFA World Cup, meaning that its best application would be with other soccer-related tweets.\n",
        "\n",
        "* **Next Steps**:\n",
        "  * Implement a pre-trained embedding layer\n",
        "  * Add more data to combat overfitting"
      ],
      "metadata": {
        "id": "g6UAH9-bOM2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg2ALngIHZUX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(33)"
      ],
      "metadata": {
        "id": "bzs2r1SSiFsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face dataset\n",
        "fifa_tweets = load_dataset(\"Tirendaz/fifa-world-cup-2022-tweets\")"
      ],
      "metadata": {
        "id": "aYGe0-LNpH55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_tweets = fifa_tweets[\"train\"].to_pandas() # only has a train split\n",
        "fifa_tweets.head()"
      ],
      "metadata": {
        "id": "2XLWiREQpWsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"This dataset has {fifa_tweets.shape[0]} rows and {fifa_tweets.shape[1]} features.\")"
      ],
      "metadata": {
        "id": "rUDchWStLDSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant features\n",
        "fifa_tweets.drop(axis=1, columns=[\"Unnamed: 0\", \"Date Created\", \"Number of Likes\", \"Source of Tweet\"], inplace=True)\n",
        "fifa_tweets.head()"
      ],
      "metadata": {
        "id": "JrzhIzCQJNpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA\n",
        "\n",
        "* Checking for duplicates in data and other anomalies\n",
        "* Looking at distribution of label values"
      ],
      "metadata": {
        "id": "zdolubzIj4KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_tweets.info()"
      ],
      "metadata": {
        "id": "k04wYBOd_ygP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing values"
      ],
      "metadata": {
        "id": "ExuLq3qaR4L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_tweets.describe()"
      ],
      "metadata": {
        "id": "OZ6uzulHRbEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like there are some duplicate tweets, let's handle those."
      ],
      "metadata": {
        "id": "f-rLCTvdR0MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_tweets[fifa_tweets.duplicated()]"
      ],
      "metadata": {
        "id": "JAidvi6ISIOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_tweets.drop_duplicates(inplace=True)\n",
        "fifa_tweets.shape"
      ],
      "metadata": {
        "id": "WRnV1ukETE4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = fifa_tweets['Sentiment'].value_counts()\n",
        "\n",
        "plt.bar(x=sentiment_counts.index, height=sentiment_counts.values)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pxZS_E7cAcof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows a good amount of each sentiment, but significantly less negative. This imbalance could be an issue by creating a bias in the model towards positive and neutral sentiments. We can account for this in training using class weights."
      ],
      "metadata": {
        "id": "biZjyPJdLJtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "* Clean tweets to get rid of noise in the data\n",
        "* Pass tweets into a tokenizer\n",
        "* Pad tweets so that they are all the same size input\n",
        "* Change sentiment values to integers for training"
      ],
      "metadata": {
        "id": "TCYZesYZjuU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking what kind of noise is in the tweets.\n",
        "fifa_tweets.sample(5)"
      ],
      "metadata": {
        "id": "qP16cz26GEfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_tweet(text: str):\n",
        "    \"\"\"\n",
        "    Cleans tweet to prepare for model training by getting rid of noisy text.\n",
        "\n",
        "    - Removes all the '#' in the hashtags, keeping the text\n",
        "    - Removes all the Web addresses\n",
        "    - Removes all new line characters\n",
        "    - Removes all digits\n",
        "    - Removes punctuation (including hashtags)\n",
        "    - Replaces user mentions (@___) with 'user'\n",
        "\n",
        "    Returns the cleaned text.\n",
        "    \"\"\"\n",
        "    # Remove web addresses\n",
        "    cleaned_text = re.sub(r'https?://\\S+', '', text)\n",
        "    # Removes new line characters\n",
        "    cleaned_text = re.sub(r'\\n', '', cleaned_text)\n",
        "    # Removes digits\n",
        "    cleaned_text = re.sub(r'\\d', '', cleaned_text)\n",
        "    # Removes punctuation (including hashtags)\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
        "    # Replaces user mentions\n",
        "    cleaned_text = re.sub(r'@\\w+', 'user', cleaned_text)\n",
        "\n",
        "    return cleaned_text.lower().strip()\n"
      ],
      "metadata": {
        "id": "AS95ZcE9LsQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the reviews\n",
        "fifa_tweets['Tweet'] = fifa_tweets['Tweet'].apply(clean_tweet)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>') # only consider the 10,000 most frequent words\n",
        "tokenizer.fit_on_texts(fifa_tweets[\"Tweet\"])\n",
        "sequences = tokenizer.texts_to_sequences(fifa_tweets[\"Tweet\"])\n",
        "\n",
        "# Pad sequences so that they are all same length to feed into model\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# Convert sentiment labels to integers\n",
        "sentiment_map = {\n",
        "    'positive': 2,\n",
        "    'neutral': 1,\n",
        "    'negative': 0\n",
        "}\n",
        "fifa_tweets[\"Sentiment\"] = fifa_tweets[\"Sentiment\"].map(sentiment_map)"
      ],
      "metadata": {
        "id": "Lmiy3JoUO_2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "* Split data into training-test sets (validation set comes into play in the .fit method)\n",
        "\n",
        "* RNN architecture  \n",
        "  1. **Embedding layer**  \n",
        "  2. **Bidirectional Long-Short Term Memory Layer (LSTM)**  \n",
        "  3. **Dropout layer** (for regularization)   \n",
        "  4. **Bidirectional LSTM Layer**  \n",
        "  5. **Dropout Layer**  \n",
        "  6. **Dense Layer** with ReLU activation\n",
        "  7. **Dropout Layer**\n",
        "  8. **Output Dense Layer** with Softmax activation  "
      ],
      "metadata": {
        "id": "IK5UdQ-SjnrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_sequences\n",
        "y = fifa_tweets[\"Sentiment\"].values\n",
        "\n",
        "# Splitting into training and testing splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=33)"
      ],
      "metadata": {
        "id": "lx5uIoKIV1s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Model Architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(10000, 32),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Wait 3 epochs with no improvement and then stop training\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    restore_best_weights=True # use best version of the model\n",
        ")\n",
        "\n",
        "# To account for class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Model fitting\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GgrNZKBLYtet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tested out a bunch of different architectures here. Changed learning rate from th default 0.1 because model converged way too quickly, hinting at overfitting. Also, tried more neurons in each layer but that seemed to make the model too complex and didn't really alter performance."
      ],
      "metadata": {
        "id": "B_zQVOrD-S7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training vs Validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hHAx8UvDcZ4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training vs Validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j7M3oTaHNs_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the validation plots, the model converges somewhat quickly, in only the first few epochs of training. Adding more data would probably help to change this, since there are currently only around 20,000 tweets in the dataset. Luckily, the model is restored to its best version by using the `EarlyStopping` function"
      ],
      "metadata": {
        "id": "GAFGjCKW9nh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation"
      ],
      "metadata": {
        "id": "7_fmx3h0jgzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.3f}')\n",
        "print(f'Test Loss: {test_loss:.3f}')"
      ],
      "metadata": {
        "id": "UBclo9vxhf5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Model"
      ],
      "metadata": {
        "id": "TfDya1cAjVLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tweet: str):\n",
        "  \"\"\"\n",
        "  Using the network, predicts the sentiment of a soccer tweet.\n",
        "\n",
        "  - Preprocessed tweet first\n",
        "    - tokenize and pad\n",
        "  - Predict with model.predict() and change label from integer to sentiment\n",
        "\n",
        "  Returns the predicted sentiment.\n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_tweet = clean_tweet(tweet)\n",
        "\n",
        "  sample_sequence = tokenizer.texts_to_sequences([cleaned_tweet])\n",
        "  sample_padded = pad_sequences(sample_sequence, maxlen=100)\n",
        "\n",
        "  reverse_sentiment_map = {\n",
        "      2: \"Positive\",\n",
        "      1: \"Neutral\",\n",
        "      0: \"Negative\"\n",
        "  }\n",
        "\n",
        "  # Returns the raw softmax probabilities\n",
        "  prediction = model.predict(sample_padded)\n",
        "  # Selects highest probability\n",
        "  class_prediction = reverse_sentiment_map[np.argmax(prediction)]\n",
        "\n",
        "  return class_prediction"
      ],
      "metadata": {
        "id": "28H_M1EVdvZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews = [\n",
        "    \"I'm not a huge fan of watching Ronaldo play. Hopefully Portugal loses.\",\n",
        "    \"Soccer is super fun, and I really hope Argentina wins!\"\n",
        "]\n",
        "\n",
        "for review in sample_reviews:\n",
        "  prediction = predict(review)\n",
        "\n",
        "  print(f\"Review: {review}\")\n",
        "  print(f\"Prediction: {prediction}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "STK62trPiNkd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}